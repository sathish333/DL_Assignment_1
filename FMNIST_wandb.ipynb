{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9afe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_loss='cross_entropy_loss'\n",
    "squared_loss='mean_squared_error'\n",
    "\n",
    "fmnist_dataset='fashion_mnist'\n",
    "mnist_dataset='mnist'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1a1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist,mnist\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "\n",
    "class Activation_Functions:\n",
    "    def __init__(self,function):\n",
    "        self.function=function\n",
    "    def getActivations(self,x):\n",
    "        if self.function=='relu':\n",
    "            return np.maximum(0,x)\n",
    "        elif self.function=='sigmoid':\n",
    "            return 1.0/(1+np.exp(-x))\n",
    "            return sigmoid(x)\n",
    "        elif self.function=='tanh':\n",
    "            return np.tanh(x)\n",
    "    def getDerivatives(self,x):\n",
    "        if self.function=='relu':\n",
    "             return 1*(x>0)\n",
    "        elif self.function=='sigmoid':\n",
    "            return sigmoid(x)*(1-sigmoid(x))\n",
    "        elif self.function=='tanh':\n",
    "            return (1 - (np.tanh(x)**2))         \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def reluPrime(x):\n",
    "    return 1*(x>0)\n",
    "\n",
    "class Optimizers:\n",
    "    def __init__(self,method):\n",
    "        self.method=method\n",
    "    def update(self,obj,x_batch=None,y_batch=None):\n",
    "        \n",
    "        lr=obj.params.learning_rate\n",
    "        momentum=0.9\n",
    "        adam_beta1=0.9\n",
    "        adam_beta2=0.99\n",
    "        rms_beta=0.9\n",
    "        weight_decay=obj.params.weight_decay\n",
    "        epsilon=1e-9\n",
    "        \n",
    "\n",
    "        if self.method=='sgd':\n",
    "            for i in range(len(obj.layers)):\n",
    "                obj.weights[i]=obj.weights[i]-lr*(2*weight_decay*obj.weights[i]+obj.gradients_w[i])\n",
    "                obj.biases[i]=obj.biases[i]-lr*obj.gradients_b[i]\n",
    "        elif self.method=='momentum':\n",
    "            init=0\n",
    "            new_gradients_w=[None]*len(obj.gradients_w)\n",
    "            new_gradients_b=[None]*len(obj.gradients_b)\n",
    "            for i in range(len(obj.layers)):\n",
    "                if obj.prev_gradients_w!=None:\n",
    "                    new_gradients_w[i]=(1-momentum)*obj.gradients_w[i]+obj.prev_gradients_w[i]*momentum\n",
    "                    new_gradients_b[i]=(1-momentum)*obj.gradients_b[i]+obj.prev_gradients_b[i]*momentum\n",
    "                    \n",
    "                else:\n",
    "                    new_gradients_w[i]=lr*obj.gradients_w[i]\n",
    "                    new_gradients_b[i]=lr*obj.gradients_b[i]\n",
    "                    \n",
    "                obj.weights[i]=obj.weights[i]-lr*(new_gradients_w[i]+2*weight_decay*obj.weights[i])\n",
    "                obj.biases[i]=obj.biases[i]-lr*(new_gradients_b[i])\n",
    "                \n",
    "            obj.prev_gradients_w=copy.deepcopy(new_gradients_w)\n",
    "            obj.prev_gradients_b=copy.deepcopy(new_gradients_b)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            pass\n",
    "        elif self.method=='rmsprop':\n",
    "            for i in range(len(obj.layers)):\n",
    "                obj.rms_v[i]=rms_beta*obj.rms_v[i]+(1-rms_beta)*(obj.gradients_w[i]**2)\n",
    "                \n",
    "                obj.weights[i]=obj.weights[i]-(lr/(np.sqrt(obj.rms_v[i])+epsilon))*(obj.gradients_w[i])\n",
    "                 \n",
    "                obj.biases[i]=obj.biases[i]-lr*obj.gradients_b[i] \n",
    "                \n",
    "        elif self.method=='adam':\n",
    "            for i in range(len(obj.layers)):\n",
    "                obj.adam_m[i]=adam_beta1*obj.adam_m[i]+(1-adam_beta1)*obj.gradients_w[i]\n",
    "                m_hat=obj.adam_m[i]/(1-np.power(adam_beta1,obj.counter))\n",
    "                \n",
    "                obj.adam_v[i]=adam_beta2*obj.adam_v[i]+(1-adam_beta2)*obj.gradients_w[i]*obj.gradients_w[i]\n",
    "                v_hat=obj.adam_v[i]/(1-np.power(adam_beta2,obj.counter))\n",
    "                obj.weights[i]=obj.weights[i]-(lr/(np.sqrt(v_hat)+epsilon))*m_hat\n",
    "                obj.biases[i]=obj.biases[i]-lr*obj.gradients_b[i]\n",
    "        \n",
    "        elif self.method=='nadam':\n",
    "             for i in range(len(obj.layers)):\n",
    "                obj.adam_m[i]=adam_beta1*obj.adam_m[i]+(1-adam_beta1)*obj.gradients_w[i]\n",
    "                m_hat=obj.adam_m[i]/(1-adam_beta1)\n",
    "                \n",
    "                obj.adam_v[i]=adam_beta2*obj.adam_v[i]+(1-adam_beta2)*obj.gradients_w[i]*obj.gradients_w[i]\n",
    "                v_hat=obj.adam_v[i]/(1-adam_beta2)\n",
    "                obj.weights[i]=obj.weights[i]-(lr/(np.sqrt(v_hat)+epsilon))*m_hat\n",
    "                \n",
    "                obj.weights[i]=obj.weights[i]-(lr*(np.divide((adam_beta1*m_hat)+((1-adam_beta1)/(1-np.power(adam_beta1,obj.counter)))*obj.gradients_w[i],np.sqrt(v_hat)+1e-9)))\n",
    "                obj.biases[i]=obj.biases[i]-lr*obj.gradients_b[i]\n",
    "        elif self.method=='nag':\n",
    "            original_weights=copy.deepcopy(obj.weights)\n",
    "            original_biases=copy.deepcopy(obj.biases)\n",
    "            original_graidents_w=copy.deepcopy(obj.gradients_w)\n",
    "            original_graidents_b=copy.deepcopy(obj.gradients_b)\n",
    "            \n",
    "            if obj.prev_gradients_w!=None:\n",
    "                 for i in range(len(obj.layers)):\n",
    "                        obj.weights[i]=obj.weights[i]-momentum*obj.gradients_w[i]\n",
    "                        obj.biases[i]=obj.biases[i]-momentum*obj.gradients_b[i]\n",
    "            \n",
    "            layer_outs,inter_values=obj.forward(x_batch)\n",
    "            loss=0\n",
    "            if(obj.loss_function==entropy_loss):\n",
    "                loss=cross_entropy_loss(layer_outs[-1],y_batch)\n",
    "            elif(obj.loss_function==squared_loss):\n",
    "                loss=cross_entropy_loss(layer_outs[-1],y_batch)\n",
    "            obj.compute_deltas(layer_outs,inter_values,y_batch)\n",
    "            obj.find_gradients(x_batch,layer_outs)\n",
    "            \n",
    "            for i in range(len(obj.gradients_w)):\n",
    "                if obj.prev_gradients_w!=None:\n",
    "                    obj.gradients_w[i]=original_graidents_w[i]*momentum+obj.gradients_w[i]\n",
    "                    obj.gradients_b[i]=original_graidents_b[i]*momentum+obj.gradients_b[i]\n",
    "                    \n",
    "                obj.weights[i]=original_weights[i]-lr*(obj.gradients_w[i]+2*weight_decay*obj.weights[i])\n",
    "                obj.biases[i]=original_biases[i]-lr*(obj.gradients_b[i])\n",
    "            obj.prev_gradients_w=True\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    x=np.clip(x,-500,500)\n",
    "    return 1.0/(1.0+1.0*np.exp(-x))\n",
    "\n",
    "    \n",
    "def sigmoidPrime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanhPrime(x):\n",
    "    return (1 - (np.tanh(x)**2))\n",
    "\n",
    "def intialize_weights(method,rows,cols):\n",
    "    if method=='uniform':\n",
    "        return np.random.rand(rows,cols)\n",
    "    elif method=='xavier':\n",
    "        return np.random.randn(rows,cols)*np.sqrt(2/(rows+cols))\n",
    "    else:\n",
    "        return np.random.randn(rows,cols)\n",
    "\n",
    "def soft_max(x):\n",
    "    max_=np.max(x,axis=0)\n",
    "    x=x-max_\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "def soft_max_prime(x):\n",
    "    return soft_max(x)*(1-soft_max(x))\n",
    "    \n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self,max_):\n",
    "        self.max_=max_\n",
    "    def transform(self,x):\n",
    "        out=np.zeros((len(x),self.max_))\n",
    "        out[np.arange(len(x)),x]=1\n",
    "        return out.T\n",
    "\n",
    "def cross_entropy_loss(y_pre,y):\n",
    "    y_pre[y_pre<1e-15]=1e-15\n",
    "    loss=-np.sum(y*np.log(y_pre))\n",
    "    return loss/float(y_pre.shape[1])\n",
    "\n",
    "def mean_squared_loss(y_pre,y):\n",
    "    return np.mean((y_pre-y)**2)\n",
    "\n",
    "def getSig():\n",
    "    return lambda x:1.0/(1+np.exp(-x))\n",
    "\n",
    "def pre_process(x):\n",
    "    x=x.reshape(-1,784)\n",
    "    x=x/255\n",
    "    return x\n",
    "\n",
    "class NN:\n",
    "    def __init__(self,input_size,layers,params):\n",
    "        self.input_size=input_size\n",
    "        self.layers=layers\n",
    "        self.nlayers=len(layers)\n",
    "        self.activations=Activation_Functions(params.activation)\n",
    "        self.optimizer=Optimizers(params.optimizer)\n",
    "        self.weights=[]\n",
    "        self.weights_init=params.weight_init\n",
    "        self.weights.append(intialize_weights(self.weights_init,layers[0],input_size))\n",
    "        self.deltas=[None]*self.nlayers\n",
    "        self.gradients_w=[None]*self.nlayers\n",
    "        self.gradients_b=[None]*self.nlayers\n",
    "        self.prev_gradients_w=None\n",
    "        self.pre_gradients_b=None\n",
    "        self.loss_function=params.loss\n",
    "        self.counter=1\n",
    "        self.biases=[]\n",
    "        self.batch_size=params.batch_size\n",
    "        for i in range(1,len(layers)):\n",
    "            self.weights.append(intialize_weights(self.weights_init,layers[i],layers[i-1]))\n",
    "        for i in range(len(layers)):\n",
    "            self.biases.append(np.random.randn(layers[i],1))   \n",
    "        \n",
    "        if self.optimizer.method in ['adam','nadam']:\n",
    "            self.adam_m=copy.deepcopy(self.weights)\n",
    "            self.adam_v=copy.deepcopy(self.adam_m)\n",
    "            _=[each.fill(0) for each in self.adam_m]\n",
    "            _=[each.fill(0) for each in self.adam_v]\n",
    "        if self.optimizer.method=='rmsprop':\n",
    "            self.rms_v=copy.deepcopy(self.weights)\n",
    "            _=[each.fill(0) for each in self.rms_v]\n",
    "        self.params=params\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        layer_outputs=[]\n",
    "        inter_values=[]\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            inter_values.append(self.weights[i].dot(x)+self.biases[i])\n",
    "            if i!= len(self.weights)-1:\n",
    "                layer_outputs.append(self.activations.getActivations(inter_values[-1]))\n",
    "                x=layer_outputs[-1]\n",
    "        layer_outputs.append(soft_max(inter_values[-1]))\n",
    "        return layer_outputs,inter_values\n",
    "    def compute_deltas(self,layer_outputs,inter_values,y_one_hot):\n",
    "        if(self.loss_function==entropy_loss):\n",
    "            self.deltas[self.nlayers-1]=(layer_outputs[self.nlayers-1]-y_one_hot)\n",
    "        elif(self.loss_function==squared_loss):\n",
    "            self.deltas[self.nlayers-1] = (layer_outputs[self.nlayers-1]-y_one_hot) * soft_max_prime(inter_values[self.nlayers-1])\n",
    "            \n",
    "        for i in range(self.nlayers-2,-1,-1):\n",
    "            self.deltas[i]=np.matmul(self.weights[i+1].T,self.deltas[i+1])*self.activations.getDerivatives(inter_values[i])\n",
    "        \n",
    "    def find_gradients(self,x,layer_outs):\n",
    "        self.gradients_w[0]=(np.matmul(self.deltas[0],x.T)+2*self.params.weight_decay*self.weights[0])/self.batch_size\n",
    "        for i in range(1,self.nlayers):\n",
    "            self.gradients_w[i]=(np.dot(self.deltas[i],layer_outs[i-1].T)+2*self.params.weight_decay*self.weights[i])/self.batch_size\n",
    "        for i in range(self.nlayers):\n",
    "            self.gradients_b[i]=np.sum(self.deltas[i],keepdims=True)/self.batch_size\n",
    "             \n",
    "    def update_weights(self,lr):\n",
    "        for each in range(self.nlayers):\n",
    "            self.optimizer.update(self)\n",
    "    \n",
    "            \n",
    "    def train(self,x,y,x_valid,y_valid):\n",
    "        lr=self.params.learning_rate\n",
    "        batch_size=self.params.batch_size\n",
    "        train_losses=[]\n",
    "        valid_losses=[] \n",
    "        epochs=self.params.epochs\n",
    "        for epoch in range(epochs):                \n",
    "            i=0\n",
    "            batch_count=0\n",
    "            loss=0.0\n",
    "            while i+batch_size < x.shape[1]:\n",
    "                batch_count+=1\n",
    "                x_batch=x[:,i:i+batch_size]\n",
    "                y_batch=y[:,i:i+batch_size]\n",
    "                i+=batch_size\n",
    "                if self.optimizer.method!='nag':\n",
    "                    layer_outs,inter_values=self.forward(x_batch)\n",
    "                    if(self.loss_function==entropy_loss):\n",
    "                        loss+=cross_entropy_loss(layer_outs[-1],y_batch)\n",
    "                    elif(self.loss_function==squared_loss):\n",
    "                        loss+=mean_squared_loss(layer_outs[-1],y_batch)\n",
    "                    self.compute_deltas(layer_outs,inter_values,y_batch)\n",
    "                    self.find_gradients(x_batch,layer_outs)\n",
    "                    self.optimizer.update(self)\n",
    "                else:\n",
    "                    loss+=self.optimizer.update(self,x_batch=x_batch,y_batch=y_batch)\n",
    "                self.counter+=1\n",
    "                    \n",
    "            train_losses.append(loss/batch_count)\n",
    "            layer_outs,inter_values=self.forward(x_valid)\n",
    "            if(self.loss_function==entropy_loss):\n",
    "                valid_loss=cross_entropy_loss(layer_outs[-1],y_valid)\n",
    "            else:\n",
    "                valid_loss=mean_squared_loss(layer_outs[-1],y_valid)\n",
    "                \n",
    "            valid_losses.append(valid_loss)\n",
    "            tr_ac=self.calc_accuracy(x,y)\n",
    "            val_ac=self.calc_accuracy(x_valid,y_valid)\n",
    "            print(f\"epoch {epoch+1} : train loss = {train_losses[-1]} valid loss = {valid_loss}\")\n",
    "            wandb.log({'train loss':train_losses[-1],'train accuracy':tr_ac,'valid loss':valid_loss,'valid accuracy':val_ac})\n",
    "        return train_losses,tr_ac,valid_losses,val_ac\n",
    "    \n",
    "    def predict_probas(self,x):\n",
    "        layer_outs,inter_values=self.forward(x)\n",
    "        return layer_outs[-1]\n",
    "        \n",
    "    def calc_accuracy(self,x,y):\n",
    "        layer_outs,inter_values=self.forward(x)\n",
    "        pred=np.argmax(layer_outs[-1],axis=0)\n",
    "        expected=np.argmax(y,axis=0)\n",
    "        \n",
    "        return (np.sum(pred==expected)/y.shape[1])*100\n",
    "               \n",
    "\n",
    "def load_data(dataset=fmnist_dataset):\n",
    "    if dataset==fmnist_dataset:\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    elif dataset==mnist_dataset:\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    split_size=0.8\n",
    "\n",
    "    x_train,x_valid=x_train[:int(len(x_train)*split_size)],x_train[int(len(x_train)*split_size):] #splitting train into train and valid\n",
    "    y_train,y_valid=y_train[:int(len(y_train)*split_size)],y_train[int(len(y_train)*split_size):]\n",
    "\n",
    "    x_train=pre_process(x_train)\n",
    "    x_valid=pre_process(x_valid)\n",
    "    x_test=pre_process(x_test) \n",
    "    \n",
    "    one_hot=OneHotEncoder(10)\n",
    "    y_train=one_hot.transform(y_train)\n",
    "    y_valid=one_hot.transform(y_valid)\n",
    "    y_test=one_hot.transform(y_test)\n",
    "    \n",
    "    \n",
    "    return x_train,y_train,x_valid,y_valid,x_test,y_test\n",
    "\n",
    "x_train,y_train,x_valid,y_valid,x_test,y_test=load_data(dataset=fmnist_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144a2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2z1sl0ve\n",
      "Sweep URL: https://wandb.ai/cs22m080/DL_Assign_1/sweeps/2z1sl0ve\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random,#bayes\n",
    "    'name' : 'Random_sweep_cross_entropy',\n",
    "    'metric': {\n",
    "      'name': 'valid accuracy',\n",
    "      'goal': 'maximize'  \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [10,15]\n",
    "        },\n",
    "        'num_layers':{\n",
    "            'values':[3,4,5]\n",
    "        },\n",
    "         'hidden_size':{\n",
    "            'values':[32,64,128]\n",
    "        },\n",
    "        'weight_decay':{\n",
    "            'values':[0,0.0005,0.5]\n",
    "        },\n",
    "         'learning_rate':{\n",
    "            'values':[0.1,0.01,0.001]\n",
    "        },\n",
    "         'optimizer': {\n",
    "            'values': ['sgd','momentum','nag','rmsprop','adam','nag']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16,32,64]\n",
    "        },\n",
    "         'weight_init':{\n",
    "            'values':['normal','random','xavier']\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['sigmoid','tanh','relu']\n",
    "        }, \n",
    "        'loss': {\n",
    "            'values': [entropy_loss]\n",
    "        }, \n",
    "        \n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assign_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5e41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jnzzjn2r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy_loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: normal\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with wandb.init() as run:\n",
    "        params={}\n",
    "        params=dict(wandb.config)\n",
    "        params=SimpleNamespace(**params)\n",
    "        \n",
    "        layers=[params.hidden_size]*params.num_layers\n",
    "        run_name=\"opt_\"+wandb.config.optimizer+\"-ac_\"+wandb.config.activation+\"-batch_\"+str(wandb.config.batch_size)\\\n",
    "                + \"-nlayers_\"+str(wandb.config.num_layers)+\"-epochs_\"+str(wandb.config.epochs)\n",
    "        wandb.run.name=run_name\n",
    "        layers.append(10)\n",
    "        obj=NN(784,layers,params)\n",
    "        train_losses,tr_ac,valid_losses,val_ac=obj.train(x_train.T,y_train,x_valid.T,y_valid)\n",
    "    \n",
    "wandb.agent(sweep_id, function=main,count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425bd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
